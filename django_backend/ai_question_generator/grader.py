"""
AI Auto-Grader - ä½¿ç”¨ Gemini AI è‡ªåŠ¨è¯„åˆ†
Djangoé›†æˆç‰ˆæœ¬ - ä»…åŒ…å«æ ¸å¿ƒè¯„åˆ†é€»è¾‘ï¼Œæ‰€æœ‰æ•°æ®é€šè¿‡APIä¼ è¾“
"""
import os
import json
import re
from typing import List, Dict
import google.generativeai as genai

# ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')


class AutoGrader:
    """AI è‡ªåŠ¨è¯„åˆ†å™¨"""
    
    def __init__(self, api_key: str = None):
        """åˆå§‹åŒ–è¯„åˆ†å™¨"""
        self.api_key = api_key or GEMINI_API_KEY
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° GEMINI_API_KEYï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®")
        
        # é…ç½® Gemini
        genai.configure(api_key=self.api_key)
        
        # ä½¿ç”¨ç»è¿‡æµ‹è¯•çš„å¯ç”¨æ¨¡å‹ï¼Œé…ç½®ç”Ÿæˆå‚æ•°ä»¥æé«˜ä¸€è‡´æ€§
        self.model = genai.GenerativeModel(
            'gemini-2.5-flash',
            generation_config={
                'temperature': 0.1,
                'top_p': 0.8,
                'top_k': 10,
            }
        )
    
    def grade_mcq(self, question: Dict, student_answer: str) -> Dict:
        """
        è¯„åˆ†é€‰æ‹©é¢˜(ç›´æ¥æ¯”å¯¹)
        
        Args:
            question: é¢˜ç›®ä¿¡æ¯
            student_answer: å­¦ç”Ÿç­”æ¡ˆ
        
        Returns:
            è¯„åˆ†ç»“æœ
        """
        correct_answer = question.get('correct_answer', '').strip().upper()
        student_answer_clean = student_answer.strip().upper()
        
        # æå–é€‰é¡¹å­—æ¯(å¤„ç† "A. ..." æˆ– "A" æ ¼å¼)
        if '.' in student_answer_clean:
            student_answer_clean = student_answer_clean.split('.')[0].strip()
        if '.' in correct_answer:
            correct_answer = correct_answer.split('.')[0].strip()
        
        is_correct = student_answer_clean == correct_answer
        # ğŸ”¥ å¼ºåˆ¶æ¯é¢˜10åˆ†
        max_score = 10
        score = max_score if is_correct else 0
        
        return {
            'question_id': question.get('id'),
            'type': 'mcq',
            'student_answer': student_answer,
            'correct_answer': correct_answer,
            'is_correct': is_correct,
            'score': score,
            'max_score': max_score,
            'feedback': question.get('explanation', '') if is_correct else f"Incorrect. The correct answer is {correct_answer}. {question.get('explanation', '')}"
        }
    
    def grade_short_answer(self, question: Dict, student_answer: str, rubric: Dict = None) -> Dict:
        """
        ä½¿ç”¨ AI è¯„åˆ†ç®€ç­”é¢˜
        
        Args:
            question: é¢˜ç›®ä¿¡æ¯
            student_answer: å­¦ç”Ÿç­”æ¡ˆ
            rubric: è¯„åˆ†ç»†åˆ™(å¯é€‰ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†æ ‡å‡†)
        
        Returns:
            è¯„åˆ†ç»“æœ
        """
        # æ„å»ºè¯„åˆ†æç¤ºè¯
        prompt = self._build_grading_prompt(question, student_answer)
        
        try:
            # è°ƒç”¨ Gemini API
            response = self.model.generate_content(prompt)
            
            # è§£æè¯„åˆ†ç»“æœ
            result = self._parse_grading_response(response.text, question, student_answer)
            
            return result
            
        except Exception as e:
            # è¿”å›é»˜è®¤è¯„åˆ†ï¼ŒğŸ”¥ å¼ºåˆ¶10åˆ†æ»¡åˆ†
            return {
                'question_id': question.get('id'),
                'type': 'short_answer',
                'student_answer': student_answer,
                'score': 0,
                'max_score': 10,
                'feedback': f'Grading failed: {str(e)}',
                'breakdown': {}
            }
    
    def _build_grading_prompt(self, question: Dict, student_answer: str) -> str:
        """æ„å»ºè¯„åˆ†æç¤ºè¯"""
        
        # ğŸ”¥ å¼ºåˆ¶æ¯é¢˜10åˆ†æ»¡åˆ†
        max_score = 10
        key_points = question.get('grading_points', [])
        key_points_text = "\n".join(f"- {p}" for p in key_points)
        sample_answer = question.get('sample_answer', 'Not provided')
        
        prompt = f"""You are a CONSISTENT and OBJECTIVE grader for educational assessments. You must grade deterministically - identical answers should always receive identical scores.

**CRITICAL GRADING RULES**:
1. Use the detailed rubric below to assign scores (0-4 for each criterion)
2. Award partial credit based on specific descriptors
3. Be CONSISTENT - same answer quality = same score every time
4. Base scores ONLY on observable evidence in the student's answer
5. Generate personalized hint and solution based on student's specific mistakes

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**Question**: {question.get('question')}

**Reference Answer**: {sample_answer}

**Required Key Points**:
{key_points_text}

**Student Answer**: {student_answer}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**DETAILED GRADING RUBRIC**:

**Correctness (0-4 points)**:
- 0: Incorrect - The answer is fundamentally wrong or completely off-topic, with no meaningful overlap with the correct concepts.
- 1: Limited correctness - Only a few correct ideas appear. Several important concepts are misunderstood, mixed up, or incorrectly explained.
- 2: Partially correct - Some key ideas are correct, but parts of the answer are vague, slightly inaccurate, or somewhat confused. The overall direction is partly right but not fully reliable.
- 3: Mostly correct - Most key ideas are correct and consistent with the reference answer. There may be minor imprecision or small gaps, but no serious conceptual errors.
- 4: Fully correct - The answer is conceptually accurate and aligned with the reference answer. All mentioned key ideas are correct; no major misunderstandings or contradictions.

**Completeness (0-4 points)**:
- 0: Not complete - The answer does not meaningfully cover any of the expected key ideas.
- 1: Minimally complete - The answer mentions only a small number of expected key ideas (around 1-39%). Coverage is very partial and feels incomplete.
- 2: Partially complete - The answer covers some expected key ideas (around 40-59%). Several important ideas are missing or only hinted at.
- 3: Largely complete - The answer covers many expected key ideas (around 60-79%), but a few important ideas are missing or only briefly mentioned.
- 4: Fully complete - The answer covers almost all of the expected key ideas (around 80-100% of them). No major idea from the marking guide is missing.

**Clarity (0-2 points)**:
- 0: Hard to understand - The answer is poorly structured or very confusing. Sentences are fragmented or disorganized, with serious grammar or wording problems that make the meaning difficult to follow.
- 1: Generally understandable - The main idea can be understood, but the answer may be loosely structured, somewhat repetitive, or contain awkward phrasing and minor grammar issues.
- 2: Clear and well-structured - The answer is easy to read and follow. Sentences are coherent, the logic is sensible, and ideas are organized. Language issues, if any, do not hinder understanding.

**Total Score**: Sum of Correctness + Completeness + Clarity (Maximum: 10 points)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

**GRADING PROCESS**:

Step 1: Analyze the student answer against each criterion
Step 2: Assign scores (Correctness: 0-4, Completeness: 0-4, Clarity: 0-2)
Step 3: Calculate total_score = Correctness + Completeness + Clarity
Step 4: Identify specific gaps or errors in the student's answer
Step 5: Generate a personalized HINT that addresses the student's specific weaknesses without giving away the full answer
Step 6: Generate a SOLUTION that explains what the student missed and how to improve

**OUTPUT FORMAT** (MUST be valid JSON, no extra text):
{{
  "breakdown": {{
    "Correctness": <0-4>,
    "Completeness": <0-4>,
    "Clarity": <0-2>
  }},
  "total_score": <sum_of_above>,
  "feedback": "Detailed feedback explaining the score with specific references to what was correct/incorrect/missing",
  "hint": "Personalized hint based on student's specific mistakes (guide them without revealing the answer)",
  "solution": "Step-by-step explanation of what the student should have included and why"
}}

**HINT GENERATION GUIDELINES**:
- If student got 0-2 points: Provide fundamental concepts they need to review
- If student got 3-5 points: Point to specific missing key points
- If student got 6-8 points: Suggest refinements and additional details
- If student got 9-10 points: Offer advanced insights or alternative perspectives

**SOLUTION GENERATION GUIDELINES**:
- Explain each missing key point from grading_points
- Show how the reference answer addresses these points
- Provide concrete examples or explanations
- Structure as step-by-step guidance

**CONSTRAINTS**:
- Use numbers only (int), NO strings for scores
- Correctness: 0, 1, 2, 3, or 4
- Completeness: 0, 1, 2, 3, or 4
- Clarity: 0, 1, or 2
- total_score MUST equal sum of breakdown scores
- total_score MUST NOT exceed {max_score}
- Return ONLY the JSON object, nothing else
- Be CONSISTENT and DETERMINISTIC

Begin grading:"""
        
        return prompt
    
    def _parse_grading_response(self, response_text: str, question: Dict, student_answer: str) -> Dict:
        """è§£æ AI è¯„åˆ†å“åº”"""
        
        # æå– JSON
        cleaned = response_text.strip()
        if cleaned.startswith('```'):
            match = re.search(r'```(?:json)?\s*(\{.*\})\s*```', cleaned, re.DOTALL)
            if match:
                cleaned = match.group(1)
            else:
                cleaned = re.sub(r'^```(?:json)?[\s\n]*', '', cleaned)
                cleaned = re.sub(r'[\s\n]*```$', '', cleaned)
        
        # è§£æ JSON
        grading_result = json.loads(cleaned)
        
        # æ„å»ºæ ‡å‡†æ ¼å¼ç»“æœï¼ŒğŸ”¥ å¼ºåˆ¶max_scoreä¸º10
        return {
            'question_id': question.get('id'),
            'type': 'short_answer',
            'student_answer': student_answer,
            'score': grading_result.get('total_score', 0),
            'max_score': 10,  # å¼ºåˆ¶10åˆ†
            'feedback': grading_result.get('feedback', ''),
            'breakdown': grading_result.get('breakdown', {}),
            'hint': grading_result.get('hint', ''),
            'solution': grading_result.get('solution', '')
        }
    
    def grade_all(self, questions: List[Dict], student_answers: Dict, student_id: str = 'unknown') -> Dict:
        """
        è¯„åˆ†æ‰€æœ‰é¢˜ç›®
        
        Args:
            questions: é¢˜ç›®åˆ—è¡¨
            student_answers: å­¦ç”Ÿç­”æ¡ˆå­—å…¸ {question_id: answer}
            student_id: å­¦ç”ŸID
        
        Returns:
            å®Œæ•´è¯„åˆ†ç»“æœ
        """
        results = []
        
        for q in questions:
            qid = q.get('id')
            student_ans = student_answers.get(str(qid), '')
            
            if not student_ans:
                results.append({
                    'question_id': qid,
                    'type': q.get('type'),
                    'student_answer': '',
                    'score': 0,
                    'max_score': 10,  # ğŸ”¥ å¼ºåˆ¶10åˆ†
                    'feedback': 'No answer provided'
                })
                continue
            
            # æ ¹æ®é¢˜å‹è¯„åˆ†
            if q.get('type') == 'mcq':
                result = self.grade_mcq(q, student_ans)
            else:
                result = self.grade_short_answer(q, student_ans)
            
            results.append(result)
        
        # è®¡ç®—æ€»åˆ†
        total_score = sum(r.get('score', 0) for r in results)
        total_max = sum(r.get('max_score', 0) for r in results)
        
        return {
            'student_id': student_id,
            'grading_results': results,
            'total_score': total_score,
            'total_max_score': total_max,
            'percentage': (total_score / total_max * 100) if total_max > 0 else 0
        }
